{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_classification_rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "vLuvwRzjKasX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "Jdb_zS6RCkQQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FNrxAjHdE0Nu"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/CUAI_Winter/train.tsv\", delimiter='\\t')\n",
        "valid_df = pd.read_csv(\"/content/drive/MyDrive/CUAI_Winter/valid.tsv\", delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = [train_df['sent'][i] for i in range(len(train_df['sent']))]\n",
        "sentences_valid = [valid_df['sent'][i] for i in range(len(valid_df['sent']))]\n",
        "\n",
        "sentences_train_len = [len(train_df['sent'][i]) for i in range(len(train_df['sent']))]\n",
        "sentences_valid_len = [len(valid_df['sent'][i]) for i in range(len(valid_df['sent']))]\n",
        "\n",
        "max_train_sentence_length = max(sentences_train_len)\n",
        "max_valid_sentence_length = max(sentences_valid_len)\n",
        "\n",
        "print(max_train_sentence_length)\n",
        "print(max_valid_sentence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyfdaF_PKjeA",
        "outputId": "bec1887e-c8e2-4530-c76a-223807da2cd2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229\n",
            "146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_df['sentiment'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9prK_zdOEDx",
        "outputId": "aea78545-ce68-466d-a1d6-10cd69d7dd35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 5 3 1 4 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train = train_df['sentiment'].to_list()\n",
        "labels_valid = valid_df['sentiment'].to_list()"
      ],
      "metadata": {
        "id": "anzCkPkYO4k4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_word2vec_from_kyubyong():\n",
        "  kyubyong_word2vec = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/CUAI_Winter/ko.bin\")\n",
        "  return kyubyong_word2vec"
      ],
      "metadata": {
        "id": "fMCsM7b7Qp0D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = load_word2vec_from_kyubyong()"
      ],
      "metadata": {
        "id": "X--G2LZUR_qo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aihub 감성 말뭉치 데이터로 추가 학습\n",
        "aihub_sentences = sentences_train\n",
        "def train_word2vec_with_aihub(sentences, word2vec):\n",
        "  sentences = sentences + [['']]\n",
        "  word2vec.build_vocab(sentences, update=True)\n",
        "  word2vec.train(sentences, total_examples=word2vec.corpus_count, epochs=word2vec.epochs)\n",
        "  return "
      ],
      "metadata": {
        "id": "dyVgd4Bac4gD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word2vec_with_aihub(aihub_sentences, word2vec)"
      ],
      "metadata": {
        "id": "8z8BXKi7iorO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labels_to_indicies(labels):\n",
        "  labels_in_indicies = []\n",
        "  for label in labels:\n",
        "    label_in_indicies = [0.] * len(valid_df['sentiment'].unique())\n",
        "    label_in_indicies[label] = 1.\n",
        "    labels_in_indicies.append(label_in_indicies)\n",
        "  return labels_in_indicies"
      ],
      "metadata": {
        "id": "U-asPnZoMudD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indicies_to_labels(indicies):\n",
        "  indicies_in_labels = []\n",
        "  for index in indicies:\n",
        "    label_index = ((index == 1.).nonzero(as_tuple=True)[0])"
      ],
      "metadata": {
        "id": "zWGdhtWKcObW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_index(word):\n",
        "  if word in word2vec:\n",
        "    return word2vec.wv.vocab[word].index\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "eaQGkogeXUGp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_to_indicies(sentences):\n",
        "  sentences_of_indicies = []\n",
        "  for sentence in sentences:\n",
        "    indicies = list(map(word_to_index, sentence))\n",
        "    sentences_of_indicies.append(indicies)\n",
        "  return sentences_of_indicies"
      ],
      "metadata": {
        "id": "VQK2akntzp6l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sentences_train):\n",
        "  sentence_indicies = []\n",
        "  for i in range(len(sentences_train)):\n",
        "    number_of_additional_indicies = max_train_sentence_length - len(sentences_train[i])\n",
        "    additional_indicies = number_of_additional_indicies * [0]\n",
        "    sentence_indicies.append(sentences_train[i] + additional_indicies)\n",
        "  \n",
        "  return sentence_indicies"
      ],
      "metadata": {
        "id": "5i6oqMYX-_as"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg7tvgvysjOp",
        "outputId": "c6b002f7-0299-4ebd-ef0e-d804ef889b6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아내', '출산', '되', '신', '나']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_indicies = sentences_to_indicies(sentences_train)\n",
        "x_valid_indicies = sentences_to_indicies(sentences_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNUKNIhlq0PU",
        "outputId": "92872ed3-9e2a-4e61-cc2c-4387f31f8b82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_padded = pad_sequences(x_train_indicies)\n",
        "x_valid_padded = pad_sequences(x_valid_indicies)"
      ],
      "metadata": {
        "id": "o20Tdxa0O9V0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train_indicies = labels_to_indicies(labels_train)\n",
        "labels_valid_indicies = labels_to_indicies(labels_valid)"
      ],
      "metadata": {
        "id": "PkZuwWSsOcxJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wordvector => tensor\n",
        "def wordindicies_to_tensor():\n",
        "  return map(torch.tensor, (x_train_padded, labels_train_indicies, x_valid_padded, labels_valid_indicies))"
      ],
      "metadata": {
        "id": "B4Mtx7ZBr1xs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_valid, y_valid = wordindicies_to_tensor()"
      ],
      "metadata": {
        "id": "tW3auPLftmpb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.size())\n",
        "print(y_train.size())\n",
        "print(x_valid.size())\n",
        "print(y_valid.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2fkLesJh-Qt",
        "outputId": "75f673b6-c4e0-48d9-a5ac-505882f80ec0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40879, 229])\n",
            "torch.Size([40879, 6])\n",
            "torch.Size([5130, 229])\n",
            "torch.Size([5130, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "valid_dataset = TensorDataset(x_valid, y_valid)"
      ],
      "metadata": {
        "id": "hxCJ183ttYCO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 필요한 변수들의 정의\n",
        "learning_rate = 0.001 # 학습률 \n",
        "epochs = 10 # 전체 학습 순회 횟수\n",
        "batch_size = 64 # 훈련에 사용할 미니 데이터셋의 크기(데이터의 개수)\n",
        "dropout = 0.2\n",
        "n_classes = len(train_df['sentiment'].unique())\n",
        "loss_func = nn.functional.cross_entropy\n",
        "\n",
        "input_size = 200\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "\n",
        "weights = torch.FloatTensor(word2vec.wv.vectors)\n",
        "\n",
        "Classification_model = None"
      ],
      "metadata": {
        "id": "2nI4D07zkZ28"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader \n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size * 2, shuffle=False)"
      ],
      "metadata": {
        "id": "ns2cnUZtNwv3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단순한 시퀀스 모델보다 복잡한 모델을 구성해야 할 때는, nn.Module의 하위 클래스를 선언하고,\n",
        "# 입력 텐서를 받아 다른 모듈 및 autograd 연산을 사용하여 출력 텐서를 만드는 forward method를 정의 합니다.\n",
        "# 입력 텐서(문장)의 길이는 모두 같아야 한다.\n",
        "\n",
        "class Rnn_Sentiment_Classification(nn.Module):\n",
        "  # input_size: wordvector의 크기 200\n",
        "  # hidden_size: hidden cell의 크기 128, 256\n",
        "  def __init__(self, input_size, hidden_size, num_layers, dropout, n_classes):\n",
        "    super(Rnn_Sentiment_Classification, self).__init__()\n",
        "    # 매개변수를 생성하고 멤버 변수로 지정합니다.\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(weights, padding_idx=0)\n",
        "    # self.embedding.requires_grad = False\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.out = nn.Linear(self.hidden_size, n_classes).to(DEVICE)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    # 첫번째 히든 스테이트를 0벡터로 초기화\n",
        "    input_tensor = self.embedding(input_tensor)\n",
        "    h_0 = self.initHidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "    # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
        "    x, _ = self.gru(input_tensor, h_0)\n",
        "\n",
        "    # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
        "    h_t = x[:,-1,:]\n",
        "\n",
        "    self.dropout(h_t)\n",
        "\n",
        "    # (배치 크기, 은닉 상태의 크기) => (배치 크기, 출력층의 크기)\n",
        "    logit = self.out(h_t)\n",
        "\n",
        "    return logit\n",
        "\n",
        "  def initHidden(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.num_layers, batch_size, self.hidden_size).zero_()"
      ],
      "metadata": {
        "id": "RWibr2W_SmFm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  model = Rnn_Sentiment_Classification(input_size, hidden_size, num_layers, dropout, n_classes).to(DEVICE)\n",
        "  return model, optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def loss_batch(model, loss_func, x_batch, y_batch, opt=None):\n",
        "  loss = loss_func(model(x_batch), y_batch)\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  return loss.item(), len(x_batch)\n",
        "\n",
        "def evaluate(model, valid_dataloader, optimizer):\n",
        "  corrects, total_loss = 0, 0\n",
        "  total_accuracy = 0\n",
        "  with torch.no_grad():\n",
        "    # losses, nums = zip(*[loss_batch(model, loss_func, x_batch.to(DEVICE), y_batch.to(DEVICE)) for x_batch, y_batch in valid_dataloader])\n",
        "    # val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "    for x_batch, y_batch in valid_dataloader:\n",
        "      x_batch = x_batch.to(DEVICE)\n",
        "      y_batch = y_batch.to(DEVICE)\n",
        "      logit = model(x_batch)\n",
        "      loss = loss_func(logit, y_batch, reduction='sum')\n",
        "\n",
        "      total_loss += loss\n",
        "      correct_predictions = torch.argmax(logit, dim=1) == torch.argmax(y_batch, dim=1)\n",
        "      # corrects += (logit.max(1)[1].view(y_batch.size()).data == y_batch.data).sum()\n",
        "      accuracy = correct_predictions.float().mean()\n",
        "      total_accuracy += accuracy\n",
        "\n",
        "    size = len(valid_dataloader)\n",
        "    avg_loss = total_loss / size\n",
        "    avg_accuracy = total_accuracy / size\n",
        "\n",
        "  return avg_loss, avg_accuracy\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dataloader, valid_dataloader):\n",
        "  best_val_loss = None\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for x_batch, y_batch in train_dataloader:\n",
        "      loss_batch(model, loss_func, x_batch.to(DEVICE), y_batch.to(DEVICE), opt)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss, val_accuracy = evaluate(model, valid_dataloader, opt)\n",
        "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (epoch, val_loss, val_accuracy))\n",
        "    \n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "      torch.save(model.state_dict(), \"classification_model.pth\")\n",
        "      best_val_loss = val_loss\n",
        "\n",
        "def get_data(train_dataset, valid_dataset, batch_size):\n",
        "  return (DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(valid_dataset, batch_size=batch_size * 2, shuffle=False))"
      ],
      "metadata": {
        "id": "r2FID6NTyRQ-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수를 정의합니다.\n",
        "# optimizer를 정의합니다.\n",
        "\n",
        "def train_rnn_model(batch_size, epochs):\n",
        "  # 총 batch_size번의 학습 => 나중에 이 학습을 epochs만큼 반복할 것임.\n",
        "  # 전체 과정은 이렇다.\n",
        "  # 데이터의 미니배치를 선택\n",
        "  # 모델을 이용하여 예측 수행\n",
        "  # 손실 계산\n",
        "  # loss.backward()를 이용하여 모델의 기울기 업데이트 \n",
        "  \n",
        "  # 순전파 단계: 특정 shape의 tensor를 모델에 전달하여 예측값 y를 계산한다.\n",
        "\n",
        "  # 예측값과 실제 값을 손실함수로 전달하여 손실을 계산한다. 손실함수는 손실을 갖는 텐서를 반환한다.\n",
        "\n",
        "  # 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 초기화한다.\n",
        "\n",
        "  # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산한다.\n",
        " \n",
        "  # 경사 하강법을 사용하여 가중치를 갱신한다.\n",
        "  # with torch.no_grad():\n",
        "  #   for param in model.parameters():\n",
        "  #     param -= learning_rate * param.grad\n",
        "  # 위 3 단계를 optimizer를 정의하여 최적화한다.\n",
        "  \n",
        "  # 최종 최적화 프로세스\n",
        "  train_dataloader, valid_dataloader = get_data(train_dataset, valid_dataset, batch_size)\n",
        "  model, optimizer = get_model()\n",
        "  fit(epochs, model, loss_func, optimizer, train_dataloader, valid_dataloader)\n",
        "  Classification_model = model\n"
      ],
      "metadata": {
        "id": "llsIowqASCt7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rnn_model(batch_size, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZKDh8bizqY",
        "outputId": "d95213f8-3c92-44ac-d354-b8118897c057"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0] val loss : 221.43 | val accuracy :  0.23\n",
            "[Epoch: 1] val loss : 153.86 | val accuracy :  0.54\n",
            "[Epoch: 2] val loss : 149.81 | val accuracy :  0.55\n",
            "[Epoch: 3] val loss : 147.61 | val accuracy :  0.56\n",
            "[Epoch: 4] val loss : 148.26 | val accuracy :  0.56\n",
            "[Epoch: 5] val loss : 149.70 | val accuracy :  0.56\n",
            "[Epoch: 6] val loss : 151.64 | val accuracy :  0.57\n",
            "[Epoch: 7] val loss : 155.86 | val accuracy :  0.56\n",
            "[Epoch: 8] val loss : 157.79 | val accuracy :  0.57\n",
            "[Epoch: 9] val loss : 159.60 | val accuracy :  0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BZ1AwTfVXac3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}