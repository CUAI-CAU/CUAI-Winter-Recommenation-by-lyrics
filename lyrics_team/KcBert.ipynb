{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx2mHvhGFYBp",
        "outputId": "d6ead530-e781-4353-910e-7ff99c66fbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4 MB 14.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 526 kB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 174 kB 82.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 416 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 67.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 71.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 396 kB 82.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 77.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 76.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.6 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers pytorch_lightning emoji soynlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PwyaOUq47AqP"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Yb0113DUFE1k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_DhuR7jrxRk",
        "outputId": "3f881d64-09fc-4ec5-bbd3-7a29fe1086dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKBgb7t0hCjQ",
        "outputId": "fe1eee1b-0a47-495a-a220-4a68c4031af8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.system('CUDA_LAUNCH_BLOCKING=1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPtn_Ap6iBW2",
        "outputId": "948f35c0-9914-4e7d-a592-285c84a5e478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "# torch.backends.cudnn.enabled\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ulrBdjSwlg8E"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WF3ExyUFIx0",
        "outputId": "ab0e15c0-4a00-484e-c368-81c86500a082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN1zqYeKH2JZ"
      },
      "source": [
        "## 기본 학습 Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fPr2_vTPFuP0"
      },
      "outputs": [],
      "source": [
        "class Arg:\n",
        "    random_seed: int = 0  # Random Seed\n",
        "    pretrained_model: str = \"beomi/KcELECTRA-base\"  # Transformers PLM name\n",
        "    pretrained_tokenizer: str = \"beomi/KcELECTRA-base\"  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
        "    auto_batch_size: str = 'power'  # Let PyTorch Lightening find the best batch size \n",
        "    batch_size: int = 0  # Optional, Train/Eval Batch Size. Overrides `auto_batch_size` \n",
        "    lr: float = 5e-6  # Starting Learning Rate\n",
        "    epochs: int = 20  # Max Epochs\n",
        "    max_length: int = 150  # Max Length input size\n",
        "    report_cycle: int = 100  # Report (Train Metrics) Cycle\n",
        "    train_data_path: str = \"/content/drive/MyDrive/train.tsv\"  # Train Dataset file \n",
        "    val_data_path: str = \"/content/drive/MyDrive/valid.tsv\"  # Validation Dataset file \n",
        "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
        "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
        "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
        "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
        "    fp16: bool = False  # Enable train on FP16\n",
        "    tpu_cores: int = 0  # Enable TPU with 1 core or 8 cores\n",
        "\n",
        "args = Arg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmD2BQV9H8UQ"
      },
      "source": [
        "## 기본값을 Override 하고싶은 경우 아래와 같이 수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w2Xgj8DI1-1",
        "outputId": "871ac8e1-87e6-40c0-ee7a-a6743cbbc3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 18 06:35:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biidCBmYI3FK"
      },
      "source": [
        "위에서 GPU가 V100/P100이면 아래 `batch_size`  를 32 이상으로 하셔도 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hjwzybIrH70Y"
      },
      "outputs": [],
      "source": [
        "# args.tpu_cores = 8  # Enables TPU\n",
        "args.fp16 = True  # Enables GPU FP16\n",
        "args.batch_size = 32  # Force setup batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2AcwMYa3nmd"
      },
      "source": [
        "# Model 만들기 with Pytorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ImupuGXDGq7b"
      },
      "outputs": [],
      "source": [
        "class Model(LightningModule):\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        self.args = options\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(self.args.pretrained_model)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\n",
        "            self.args.pretrained_tokenizer\n",
        "            if self.args.pretrained_tokenizer\n",
        "            else self.args.pretrained_model\n",
        "        )\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.bert(**kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        # Transformers 4.0.0+\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "        \n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = labels.cpu().numpy()\n",
        "        y_pred = preds.cpu().numpy()\n",
        "\n",
        "        # Acc, Precision, Recall, F1\n",
        "        metrics = [\n",
        "            metric(y_true=y_true, y_pred=y_pred)\n",
        "            for metric in\n",
        "            (accuracy_score, precision_score, recall_score, f1_score)\n",
        "        ]\n",
        "\n",
        "        tensorboard_logs = {\n",
        "            'train_loss': loss.cpu().detach().numpy().tolist(),\n",
        "            'train_acc': metrics[0],\n",
        "            'train_precision': metrics[1],\n",
        "            'train_recall': metrics[2],\n",
        "            'train_f1': metrics[3],\n",
        "        }\n",
        "        if (batch_idx % self.args.report_cycle) == 0:\n",
        "            print()\n",
        "            pprint(tensorboard_logs)\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        # Transformers 4.0.0+\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = list(labels.cpu().numpy())\n",
        "        y_pred = list(preds.cpu().numpy())\n",
        "\n",
        "        \n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = torch.tensor(0, dtype=torch.float)\n",
        "        for i in outputs:\n",
        "            loss += i['loss'].cpu().detach()\n",
        "        _loss = loss / len(outputs)\n",
        "\n",
        "        loss = float(_loss)\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        for i in outputs:\n",
        "            y_true += i['y_true']\n",
        "            y_pred += i['y_pred']\n",
        "\n",
        "        # Acc, Precision, Recall, F1\n",
        "        metrics = [\n",
        "            metric(y_true=y_true, y_pred=y_pred)\n",
        "            for metric in\n",
        "            (accuracy_score, precision_score, recall_score, f1_score)\n",
        "        ]\n",
        "\n",
        "        tensorboard_logs = {\n",
        "            'val_loss': loss,\n",
        "            'val_acc': metrics[0],\n",
        "            'val_precision': metrics[1],\n",
        "            'val_recall': metrics[2],\n",
        "            'val_f1': metrics[3],\n",
        "        }\n",
        "\n",
        "        print()\n",
        "        pprint(tensorboard_logs)\n",
        "        return {'loss': _loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.args.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(self.parameters(), lr=self.args.lr)\n",
        "        elif self.args.optimizer == 'AdamP':\n",
        "            from adamp import AdamP\n",
        "            optimizer = AdamP(self.parameters(), lr=self.args.lr)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
        "        if self.args.lr_scheduler == 'cos':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "        elif self.args.lr_scheduler == 'exp':\n",
        "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
        "        else:\n",
        "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "        }\n",
        "\n",
        "    def read_data(self, path):\n",
        "        if path.endswith('xlsx'):\n",
        "            return pd.read_excel(path)\n",
        "        elif path.endswith('csv'):\n",
        "            return pd.read_csv(path)\n",
        "        elif path.endswith('tsv') or path.endswith('txt'):\n",
        "            return pd.read_csv(path, sep='\\t')\n",
        "        else:\n",
        "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
        "\n",
        "    def join_sentence(self, df):\n",
        "        df['sent'] = df['sent'].map(lambda tokens: ' '.join(tokens))\n",
        "        return df\n",
        "\n",
        "    def preprocess_dataframe(self, df):\n",
        "        # 우리는 이미 리스트로 토큰화된 문장을 각 데이터로 가지고 있으므로 이를 다시 문장으로 합쳐서 트랜스포머 토크나이저로 토큰화 하는 것이 좋아보임. \n",
        "        # 우선 메캅 토크나이저로 토큰화된 것을 가지고 트레이닝해보고 개선이 필요하면 kcElectra tokenizer로 토큰화 하기. \n",
        "        df = self.join_sentence(df)\n",
        "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
        "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
        "        url_pattern = re.compile(\n",
        "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "\n",
        "        def clean(x):\n",
        "            x = pattern.sub(' ', x)\n",
        "            x = url_pattern.sub('', x)\n",
        "            x = x.strip()\n",
        "            x = repeat_normalize(x, num_repeats=2)\n",
        "            return x\n",
        "\n",
        "        df['sent'] = df['sent'].map(lambda x: self.tokenizer.encode(\n",
        "            clean(str(x)),\n",
        "            padding='max_length',\n",
        "            max_length=self.args.max_length,\n",
        "            truncation=True,\n",
        "        ))\n",
        "        return df\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        df = self.read_data(self.args.train_data_path)\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['sent'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['sentiment'].to_list(), dtype=torch.long),\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.args.batch_size or self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.args.cpu_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        df = self.read_data(self.args.val_data_path)\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['sent'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['sentiment'].to_list(), dtype=torch.long),\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.args.batch_size or self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.args.cpu_workers,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qu1QkSZsHo8x"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Using PyTorch Ver\", torch.__version__)\n",
        "    print(\"Fix Seed:\", args.random_seed)\n",
        "    seed_everything(args.random_seed)\n",
        "    model = Model(args)\n",
        "\n",
        "    print(\":: Start Training ::\")\n",
        "    trainer = Trainer(\n",
        "        max_epochs=args.epochs,\n",
        "        fast_dev_run=args.test_mode,\n",
        "        num_sanity_val_steps=None if args.test_mode else 0,\n",
        "        auto_scale_batch_size=args.auto_batch_size if args.auto_batch_size and not args.batch_size else False,\n",
        "        # For GPU Setup\n",
        "        deterministic=torch.cuda.is_available(),\n",
        "        gpus=-1 if torch.cuda.is_available() else None,\n",
        "        precision=16 if args.fp16 else 32,\n",
        "        # For TPU Setup\n",
        "        # tpu_cores=args.tpu_cores if args.tpu_cores else None,\n",
        "    )\n",
        "    trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "8ewSOpY1Kwg6",
        "outputId": "1027f767-6682-4d54-e62c-49892e56ef6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch Ver 1.10.0+cu111\n",
            "Fix Seed: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-4fb71c39e099>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using PyTorch Ver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fix Seed:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/seed.py\u001b[0m in \u001b[0;36mseed_everything\u001b[0;34m(seed, workers)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kDZ2LEfX-YRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "KcBert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}